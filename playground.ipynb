{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloading benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /tmp/zverev/vggsound 2>/dev/null && \\\n",
    "squashfuse /storage/local/vggsound.squashfs /tmp/zverev/vggsound \n",
    "! METADATA_DIR=/tmp/zverev && \\\n",
    "MOUNTPOINT=$METADATA_DIR/vggsound && \\\n",
    "tr_data=/storage/slurm/zverev/datasets/cav-mae/vggsound/metadata/vgg_train_cleaned.json && \\\n",
    "te_data=/storage/slurm/zverev/datasets/cav-mae/vggsound/metadata/vgg_test_cleaned.json && \\\n",
    "w_data=/storage/slurm/zverev/datasets/cav-mae/vggsound/metadata/vgg_train_cleaned_weight.csv && \\\n",
    "cp $tr_data $METADATA_DIR/ && \\\n",
    "cp $te_data $METADATA_DIR/ && \\\n",
    "cp $w_data $METADATA_DIR/ && \\\n",
    "sed -i 's|/storage/slurm/zverev/datasets/cav-mae|'$MOUNTPOINT'|g' $METADATA_DIR/vgg_train_cleaned.json && \\\n",
    "sed -i 's|/storage/slurm/zverev/datasets/cav-mae|'$MOUNTPOINT'|g' $METADATA_DIR/vgg_test_cleaned.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fusermount -u /tmp/zverev/vggsound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| distributed init (rank 0): env://, gpu 0\n",
      "Using Label Smoothing: 0.1\n",
      "now using following mask: 48 freq, 192 time\n",
      "now using mix-up with rate 0.500000\n",
      "now process vggsound\n",
      "use dataset mean -5.081 and std 4.485 to normalize the input.\n",
      "now use noise augmentation\n",
      "number of classes is 309\n",
      "now in train mode.\n",
      "now use frame -1 from total 10 frames\n",
      "now using 224 * 224 image input\n",
      "Dataset has 183727 samples\n",
      "Using Label Smoothing: 0.1\n",
      "now using following mask: 48 freq, 192 time\n",
      "now using mix-up with rate 0.500000\n",
      "now process vggsound\n",
      "use dataset mean -5.081 and std 4.485 to normalize the input.\n",
      "now use noise augmentation\n",
      "number of classes is 309\n",
      "now in train mode.\n",
      "now use frame -1 from total 10 frames\n",
      "now using 224 * 224 image input\n",
      "Dataset has 183727 samples\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import src.dataloader_ft as dataloader\n",
    "import src.utils as utils\n",
    "import os\n",
    "\n",
    "# Fill args from run_videoonly.slurm\n",
    "args = type('Args', (), {})()\n",
    "args.data_train = '/tmp/zverev/vgg_train_cleaned.json'\n",
    "args.label_csv = '/storage/slurm/zverev/datasets/cav-mae/vggsound/metadata/class_labels_indices_vgg.csv'\n",
    "args.batch_size = 32\n",
    "args.num_workers = 1\n",
    "args.sql_path = '/home/wiss/zverev/AVSiam/artefacts/sql'\n",
    "args.video_path_prefix = '/tmp/zverev/vggsound/video'\n",
    "# Fill remaining args from run_videoonly.slurm\n",
    "args.target_length = 1024\n",
    "args.freqm = 48\n",
    "args.timem = 192\n",
    "args.mixup = 0.5\n",
    "args.dataset = 'vggsound'\n",
    "args.dataset_mean = -5.081\n",
    "args.dataset_std = 4.4849\n",
    "args.noise = True\n",
    "args.label_smooth = 0.1\n",
    "im_res = 224  # Standard ViT resolution\n",
    "\n",
    "args.world_size = 1\n",
    "args.local_rank = 0\n",
    "args.dist_url = 'env://'\n",
    "\n",
    "# Audio config defined from args\n",
    "audio_conf = {'num_mel_bins': 128, 'target_length': args.target_length, 'freqm': args.freqm, 'timem': args.timem, 'mixup': args.mixup,\n",
    "              'dataset': args.dataset, 'mode':'train', 'mean':args.dataset_mean, 'std':args.dataset_std,\n",
    "              'noise':args.noise, 'label_smooth': args.label_smooth, 'im_res': im_res}\n",
    "\n",
    "os.environ['RANK'] = '0'\n",
    "os.environ['WORLD_SIZE'] = '1'\n",
    "os.environ['LOCAL_RANK'] = '0'\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '12355'\n",
    "\n",
    "if not torch.distributed.is_initialized():\n",
    "    utils.init_distributed_mode(args)\n",
    "\n",
    "train_sampler = torch.utils.data.distributed.DistributedSampler(dataloader.AudiosetDataset(args.data_train, label_csv=args.label_csv, audio_conf=audio_conf, sql_path=args.sql_path, video_path_prefix=args.video_path_prefix),shuffle=True)\n",
    "train_dataset = dataloader.AudiosetDataset(args.data_train, label_csv=args.label_csv, audio_conf=audio_conf, video_path_prefix=args.video_path_prefix)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=args.batch_size, \n",
    "    sampler=train_sampler, \n",
    "    shuffle=False, \n",
    "    num_workers=args.num_workers, \n",
    "    pin_memory=True, \n",
    "    drop_last=True,\n",
    "    persistent_workers=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install line_profiler\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 0.788445 s\n",
      "File: /tmp/ipykernel_424865/3565145512.py\n",
      "Function: data_loading_benchmark at line 178\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   178                                           def data_loading_benchmark():\n",
      "   179         1        475.0    475.0      0.0      avg_time_spent = 0\n",
      "   180         1        116.0    116.0      0.0      count = 0\n",
      "   181                                               \n",
      "   182         2    1862946.0 931473.0      0.2      for index in trange(1):\n",
      "   183         1  786581567.0    8e+08     99.8          get_item(train_dataset, index)"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import trange\n",
    "import random\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torch\n",
    "import torchaudio\n",
    "import time\n",
    "\n",
    "def get_datum(self, index):\n",
    "    query = f\"SELECT * FROM annos WHERE id = {index};\"\n",
    "    res = self.cur.execute(query)\n",
    "    datum = res.fetchone()\n",
    "\n",
    "    return datum\n",
    "\n",
    "def get_item(self, index):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # SQL query timing\n",
    "    sql_start = time.time()\n",
    "    query = f\"SELECT * FROM annos WHERE id = {index};\"\n",
    "    res = self.cur.execute(query)\n",
    "    datum = res.fetchone()\n",
    "    sql_time = time.time() - sql_start\n",
    "\n",
    "    # Path construction timing\n",
    "    path_start = time.time()\n",
    "    if self.dataset == 'vggsound':\n",
    "        video_path = os.path.join(self.video_path_prefix, datum[1] + '.mp4')\n",
    "        mix_sample_idx = random.randint(0, self.num_samples-1)\n",
    "        query = f\"SELECT * FROM annos WHERE id = {mix_sample_idx};\"\n",
    "        res = self.cur.execute(query)\n",
    "        mix_datum = res.fetchone()\n",
    "        video_path_mix = os.path.join(self.video_path_prefix, mix_datum[1] + '.mp4')\n",
    "\n",
    "    elif self.dataset == 'audioset_20k':\n",
    "        if self.audio_conf['mode'] == 'train':\n",
    "            video_path = '/mnt/opr/yblin/audioset_sun/train_balanced/'+datum[1]+'.mp4'\n",
    "            mix_sample_idx = random.randint(0, self.num_samples-1)\n",
    "            query = f\"SELECT * FROM annos WHERE id = {mix_sample_idx};\"\n",
    "            res = self.cur.execute(query)\n",
    "            mix_datum = res.fetchone()\n",
    "            video_path_mix = os.path.join(self.video_path_prefix, mix_datum[1] + '.mp4')\n",
    "        else:\n",
    "            video_path = os.path.join(self.video_path_prefix, datum[1] + '.mp4')\n",
    "            mix_sample_idx = random.randint(0, self.num_samples-1)\n",
    "            query = f\"SELECT * FROM annos WHERE id = {mix_sample_idx};\"\n",
    "            res = self.cur.execute(query)\n",
    "            mix_datum = res.fetchone()\n",
    "            video_path_mix = os.path.join(self.video_path_prefix, mix_datum[1] + '.mp4')\n",
    "\n",
    "    elif self.dataset == 'audioset_2m':\n",
    "        if self.audio_conf['mode'] == 'train':\n",
    "            video_path = os.path.join(self.video_path_prefix, datum[1] + '.mp4')\n",
    "            mix_sample_idx = random.randint(0, self.num_samples-1)\n",
    "            query = f\"SELECT * FROM annos WHERE id = {mix_sample_idx};\"\n",
    "            res = self.cur.execute(query)\n",
    "            mix_datum = res.fetchone()\n",
    "            video_path_mix = os.path.join(self.video_path_prefix, mix_datum[1] + '.mp4')\n",
    "        else:\n",
    "            video_path = os.path.join(self.video_path_prefix, datum[1] + '.mp4')\n",
    "            mix_sample_idx = random.randint(0, self.num_samples-1)\n",
    "            query = f\"SELECT * FROM annos WHERE id = {mix_sample_idx};\"\n",
    "            res = self.cur.execute(query)\n",
    "            mix_datum = res.fetchone()\n",
    "            video_path_mix = os.path.join(self.video_path_prefix, mix_datum[1] + '.mp4')\n",
    "    path_time = time.time() - path_start\n",
    "\n",
    "    # Audio/video processing timing\n",
    "    if random.random() < self.mixup:\n",
    "        # get the mixed fbank\n",
    "        mix_lambda = np.random.beta(10, 10)\n",
    "\n",
    "        try:\n",
    "            fbank = self._wav2fbank(video_path, video_path_mix, mix_lambda)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            fbank = torch.zeros([self.target_length, 128]) + 0.01\n",
    "            print('there is an error in loading audio 1', datum[1],mix_datum[1])\n",
    "\n",
    "        try:\n",
    "            reader = torchvision.io.VideoReader(video_path, \"video\")\n",
    "            frames = []\n",
    "            for frame in reader:\n",
    "                frames.append(frame['data'].unsqueeze(0))\n",
    "\n",
    "            gg = torch.vstack(frames)\n",
    "            image = gg[np.linspace(random.randint(0,5), len(frames)-1, num=self.num_frame, dtype=int)]\n",
    "            image = image/255\n",
    "            image = self.my_normalize(image)\n",
    "\n",
    "            #### mixing ###\n",
    "            reader = torchvision.io.VideoReader(video_path_mix, \"video\")\n",
    "            frames = []\n",
    "            read_start = time.time()\n",
    "            for frame in reader:\n",
    "                frames.append(frame['data'].unsqueeze(0))\n",
    "\n",
    "            gg = torch.vstack(frames)\n",
    "            image2 = gg[np.linspace(random.randint(0,5), len(frames)-1, num=self.num_frame, dtype=int)]\n",
    "            image2 = image2/255\n",
    "            image2 = self.my_normalize(image2)\n",
    "\n",
    "            weight = random.random()\n",
    "            image = weight * image + (1-weight)*image2\n",
    "            image = image[random.randint(0,9)].unsqueeze(0)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            image = torch.zeros([1, 3, self.im_res, self.im_res]) + 0.01\n",
    "            print('there is an error in loading image 1', video_path, video_path_mix)\n",
    "\n",
    "        label_indices = np.zeros(self.label_num) + (self.label_smooth / self.label_num)\n",
    "        for label_str in datum[-1].split(','):\n",
    "            label_indices[int(self.index_dict[label_str])] += mix_lambda * (1.0 - self.label_smooth)\n",
    "        for label_str in mix_datum[-1].split(','):\n",
    "            label_indices[int(self.index_dict[label_str])] += (1.0 - mix_lambda) * (1.0 - self.label_smooth)\n",
    "        label_indices = torch.FloatTensor(label_indices)\n",
    "\n",
    "    else:\n",
    "        label_indices = np.zeros(self.label_num) + (self.label_smooth / self.label_num)\n",
    "        try:\n",
    "            torchaudio.load(video_path)\n",
    "            fbank = self._wav2fbank(video_path, None, 0)\n",
    "        except:\n",
    "            fbank = torch.zeros([self.target_length, 128]) + 0.01\n",
    "            print('there is an error in loading audio 2', datum)\n",
    "        try:\n",
    "            reader = torchvision.io.VideoReader(video_path, \"video\")\n",
    "            frames = []\n",
    "            for frame in reader:\n",
    "                frames.append(frame['data'].unsqueeze(0))\n",
    "\n",
    "            gg = torch.vstack(frames)\n",
    "            image = gg[np.linspace(random.randint(0,30), len(frames)-1, num=self.num_frame, dtype=int)]\n",
    "            image = image/255\n",
    "            image = self.my_normalize(image)\n",
    "\n",
    "            if self.mode =='eval':\n",
    "                pass\n",
    "            else:\n",
    "                image = image[random.randint(0,9)].unsqueeze(0)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            if self.mode =='eval':\n",
    "                image = torch.zeros([10, 3, self.im_res, self.im_res]) + 0.01\n",
    "            else:\n",
    "                image = torch.zeros([1, 3, self.im_res, self.im_res]) + 0.01\n",
    "            print('there is an error in loading image 2', video_path)\n",
    "\n",
    "        for label_str in datum[-1].split(','):\n",
    "            label_indices[int(self.index_dict[label_str])] = 1.0 - self.label_smooth\n",
    "        label_indices = torch.FloatTensor(label_indices)\n",
    "\n",
    "    # SpecAug timing\n",
    "    freqm = torchaudio.transforms.FrequencyMasking(self.freqm)\n",
    "    timem = torchaudio.transforms.TimeMasking(self.timem)\n",
    "    fbank = torch.transpose(fbank, 0, 1)\n",
    "    fbank = fbank.unsqueeze(0)\n",
    "    if self.freqm != 0:\n",
    "        fbank = freqm(fbank)\n",
    "    if self.timem != 0:\n",
    "        fbank = timem(fbank)\n",
    "    fbank = fbank.squeeze(0)\n",
    "    fbank = torch.transpose(fbank, 0, 1)\n",
    "\n",
    "    if self.skip_norm == False:\n",
    "        fbank = (fbank - self.norm_mean) / (self.norm_std)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    if self.noise == True:\n",
    "        fbank = fbank + torch.rand(fbank.shape[0], fbank.shape[1]) * np.random.rand() / 10\n",
    "        fbank = torch.roll(fbank, np.random.randint(-self.target_length, self.target_length), 0)\n",
    "    print(image.shape)\n",
    "    return fbank, image, label_indices\n",
    "\n",
    "def data_loading_benchmark():\n",
    "    avg_time_spent = 0\n",
    "    count = 0\n",
    "    \n",
    "    for index in trange(1):\n",
    "        get_item(train_dataset, index)\n",
    "\n",
    "%lprun -f data_loading_benchmark data_loading_benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Size([250, 3, 720, 1280])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fuse: mountpoint is not empty\n",
      "fuse: if you are sure this is safe, use the 'nonempty' mount option\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p /tmp/zverev/vggsound 2>/dev/null && \\\n",
    "squashfuse /storage/slurm/zverev/datasets/vggsound.squashfs /tmp/zverev/vggsound \n",
    "! METADATA_DIR=/tmp/zverev && \\\n",
    "MOUNTPOINT=$METADATA_DIR/vggsound && \\\n",
    "tr_data=/storage/slurm/zverev/datasets/cav-mae/vggsound/metadata/vgg_train_cleaned.json && \\\n",
    "te_data=/storage/slurm/zverev/datasets/cav-mae/vggsound/metadata/vgg_test_cleaned.json && \\\n",
    "w_data=/storage/slurm/zverev/datasets/cav-mae/vggsound/metadata/vgg_train_cleaned_weight.csv && \\\n",
    "cp $tr_data $METADATA_DIR/ && \\\n",
    "cp $te_data $METADATA_DIR/ && \\\n",
    "cp $w_data $METADATA_DIR/ && \\\n",
    "sed -i 's|/storage/slurm/zverev/datasets/cav-mae|'$MOUNTPOINT'|g' $METADATA_DIR/vgg_train_cleaned.json && \\\n",
    "sed -i 's|/storage/slurm/zverev/datasets/cav-mae|'$MOUNTPOINT'|g' $METADATA_DIR/vgg_test_cleaned.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 45812\n",
      "drwxr-xr-x 5 zverev tumuser        0 Oct 24 16:11 vggsound\n",
      "-rw-r--r-- 1 zverev tumuser  3284523 Jan 26 13:42 vgg_test_cleaned.json\n",
      "-rw-r--r-- 1 zverev tumuser 39028521 Jan 26 13:42 vgg_train_cleaned.json\n",
      "-rw-r--r-- 1 zverev tumuser  4593175 Jan 26 13:42 vgg_train_cleaned_weight.csv\n"
     ]
    }
   ],
   "source": [
    "!ls -l /tmp/zverev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wiss/zverev/miniconda3/envs/avsiam/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| distributed init (rank 0): env://, gpu 0\n",
      "Using Label Smoothing: 0.1\n",
      "now using following mask: 48 freq, 192 time\n",
      "now using mix-up with rate 0.500000\n",
      "now process vggsound\n",
      "use dataset mean -5.081 and std 4.485 to normalize the input.\n",
      "now use noise augmentation\n",
      "number of classes is 309\n",
      "now in train mode.\n",
      "now use frame -1 from total 10 frames\n",
      "now using 224 * 224 image input\n",
      "Dataset has 183727 samples\n",
      "Using Label Smoothing: 0.1\n",
      "now using following mask: 48 freq, 192 time\n",
      "now using mix-up with rate 0.500000\n",
      "now process vggsound\n",
      "use dataset mean -5.081 and std 4.485 to normalize the input.\n",
      "now use noise augmentation\n",
      "number of classes is 309\n",
      "now in train mode.\n",
      "now use frame -1 from total 10 frames\n",
      "now using 224 * 224 image input\n",
      "Dataset has 183727 samples\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import src.dataloader_ft as dataloader\n",
    "import src.utils as utils\n",
    "import os\n",
    "\n",
    "# Fill args from run_videoonly.slurm\n",
    "args = type('Args', (), {})()\n",
    "args.data_train = '/tmp/zverev/vgg_train_cleaned.json'\n",
    "args.label_csv = '/storage/slurm/zverev/datasets/cav-mae/vggsound/metadata/class_labels_indices_vgg.csv'\n",
    "args.batch_size = 32\n",
    "args.num_workers = 6\n",
    "args.sql_path = '/home/wiss/zverev/AVSiam/artefacts/sql'\n",
    "args.video_path_prefix = '/tmp/zverev/vggsound/video'\n",
    "# Fill remaining args from run_videoonly.slurm\n",
    "args.target_length = 1024\n",
    "args.freqm = 48\n",
    "args.timem = 192\n",
    "args.mixup = 0.5\n",
    "args.dataset = 'vggsound'\n",
    "args.dataset_mean = -5.081\n",
    "args.dataset_std = 4.4849\n",
    "args.noise = True\n",
    "args.label_smooth = 0.1\n",
    "im_res = 224  # Standard ViT resolution\n",
    "\n",
    "args.world_size = 1\n",
    "args.local_rank = 0\n",
    "args.dist_url = 'env://'\n",
    "\n",
    "# Audio config defined from args\n",
    "audio_conf = {'num_mel_bins': 128, 'target_length': args.target_length, 'freqm': args.freqm, 'timem': args.timem, 'mixup': args.mixup,\n",
    "              'dataset': args.dataset, 'mode':'train', 'mean':args.dataset_mean, 'std':args.dataset_std,\n",
    "              'noise':args.noise, 'label_smooth': args.label_smooth, 'im_res': im_res}\n",
    "\n",
    "os.environ['RANK'] = '0'\n",
    "os.environ['WORLD_SIZE'] = '1'\n",
    "os.environ['LOCAL_RANK'] = '0'\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '12355'\n",
    "\n",
    "if not torch.distributed.is_initialized():\n",
    "    utils.init_distributed_mode(args)\n",
    "\n",
    "train_sampler = torch.utils.data.distributed.DistributedSampler(dataloader.AudiosetDataset(args.data_train, label_csv=args.label_csv, audio_conf=audio_conf, sql_path=args.sql_path, video_path_prefix=args.video_path_prefix),shuffle=True)\n",
    "train_dataset = dataloader.AudiosetDataset(args.data_train, label_csv=args.label_csv, audio_conf=audio_conf, video_path_prefix=args.video_path_prefix),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "sql_path =  'artefacts/sql'\n",
    "con = sqlite3.connect(\"file:\" + f'{sql_path}/val_vgg.sqlite.db' + \"?mode=ro\", uri=True)\n",
    "# con = sqlite3.connect(\"file:\" + '/home/wiss/zverev/AVSiam/artefacts/sql/val_vgg_retrieval.sqlite.db' + \"?mode=ro\", uri=True) ### <----- YB: for retrieval\n",
    "cur = con.cursor()\n",
    "num_samples = cur.execute(\"SELECT COUNT(*) FROM annos\").fetchone()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_ids = [ row[0] for row in cur.execute(\"SELECT path FROM annos\").fetchall() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch \n",
    "import numpy as np\n",
    "import src.dataloader as dataloader\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "total_frames = 10 # change if your total frame is different\n",
    "\n",
    "label_csv = pd.read_csv('/storage/slurm/zverev/datasets/cav-mae/vggsound/metadata/class_labels_indices_vgg.csv')\n",
    "test_json = json.load(open('/storage/slurm/zverev/datasets/cav-mae/vggsound/metadata/vgg_test_cleaned.json', 'r'))[\"data\"]\n",
    "\n",
    "label_to_display_name = {\n",
    "    int(data['labels'].split('_')[1]): label_csv.loc[int(data['labels'].split('_')[1]), \"display_name\"]\n",
    "    for data in test_json\n",
    "}\n",
    "\n",
    "\n",
    "targets = np.load(f'egs/vggsound/exp/CL-test2-cav-mae-ft-5e-05-2-0.75-1-bs32-ldaFalse-mm_grad-fzFalse-h10-a5/mm_grad_target_0.npy')\n",
    "targets = targets.argmax(axis=1)\n",
    "targets = [label_to_display_name[target] for target in targets]\n",
    "\n",
    "\n",
    "modality_predictions = {}\n",
    "for modality in ['mm_grad', 'audioonly', 'videoonly']:\n",
    "    if modality == \"audioonly\":\n",
    "        target = np.load(f'egs/vggsound/exp/CL-test2-cav-mae-ft-5e-05-2-0.75-1-bs32-ldaFalse-{modality}-fzFalse-h10-a5/{modality}_target.npy')\n",
    "        target = target.argmax(axis=1)\n",
    "\n",
    "        prediction = np.load(f'egs/vggsound/exp/CL-test2-cav-mae-ft-5e-05-2-0.75-1-bs32-ldaFalse-{modality}-fzFalse-h10-a5/{modality}_output.npy')\n",
    "        multiframe_pred = prediction.mean(axis=1)\n",
    "    else:\n",
    "        target = np.load(f'egs/vggsound/exp/CL-test2-cav-mae-ft-5e-05-2-0.75-1-bs32-ldaFalse-{modality}-fzFalse-h10-a5/{modality}_target_0.npy')\n",
    "        target = target.argmax(axis=1)\n",
    "\n",
    "        multiframe_pred = []\n",
    "        for frame in range(total_frames):    \n",
    "            prediction = np.load(f'egs/vggsound/exp/CL-test2-cav-mae-ft-5e-05-2-0.75-1-bs32-ldaFalse-{modality}-fzFalse-h10-a5/{modality}_output_{frame}.npy')\n",
    "            prediction = prediction.mean(axis=1)\n",
    "            multiframe_pred.append(prediction)\n",
    "            \n",
    "        multiframe_pred = np.mean(multiframe_pred, axis=0)\n",
    "\n",
    "    multiframe_top_10 = multiframe_pred.argsort(axis=1)[:, -10:]\n",
    "    multiframe_top_1 = multiframe_top_10[:, -1]\n",
    "\n",
    "    # prepare data fo csv format\n",
    "    correctly_predicted = (multiframe_top_1 == target)\n",
    "    multiframe_top_10  = [\n",
    "        [\n",
    "            label_to_display_name[pred] if multiframe_pred[i,pred] > 1 / 309  else \"\" \n",
    "            for pred in row[::-1]\n",
    "        ] \n",
    "        for (i, row) in enumerate(multiframe_top_10)\n",
    "    ]\n",
    "    \n",
    "    #transpose multiframe_top_10\n",
    "    multiframe_top_10 = list(map(list, zip(*multiframe_top_10)))\n",
    "\n",
    "\n",
    "    # log data for the future\n",
    "    modality_predictions[modality] = {\n",
    "        'logits': multiframe_pred,\n",
    "        'correctly_predicted': correctly_predicted,\n",
    "        'multiframe_top_10': multiframe_top_10,\n",
    "    }\n",
    "\n",
    "eval_csv_data = list(zip(\n",
    "    video_ids, targets, \n",
    "    modality_predictions['audioonly']['correctly_predicted'],\n",
    "    modality_predictions['audioonly']['multiframe_top_10'][0],   \n",
    "    modality_predictions['videoonly']['correctly_predicted'],\n",
    "    modality_predictions['videoonly']['multiframe_top_10'][0],\n",
    "    modality_predictions['mm_grad']['correctly_predicted'],\n",
    "    modality_predictions['mm_grad']['multiframe_top_10'][0],\n",
    "    # top labels\n",
    "    *modality_predictions['audioonly']['multiframe_top_10'],\n",
    "    *modality_predictions['videoonly']['multiframe_top_10'],\n",
    "    *modality_predictions['mm_grad']['multiframe_top_10'],\n",
    "))\n",
    "\n",
    "columns = ['video_id', 'label', 'a', 'a_label', 'v', 'v_label', 'av', 'av_label']\n",
    "columns.extend([f'{modality}_top_{i}' for modality in ['a', 'v', 'av'] for i in range(1, 11)])\n",
    "# columns.extend([f'{modality}_top_{i}' for modality in ['av'] for i in range(1, 11)])\n",
    "\n",
    "eval_csv_pd = pd.DataFrame(eval_csv_data, columns=columns)\n",
    "eval_csv_pd.to_csv(\n",
    "    './test_predictions.csv',\n",
    "    columns=columns,\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.552505503042859, 0.5688851482584488, 0.472679010747119)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_csv_pd[\"av\"].mean(), eval_csv_pd[\"a\"].mean(), eval_csv_pd[\"v\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "modality_name_map  = {\n",
    "    'audioonly' : 'a',\n",
    "    'videoonly' : 'v',\n",
    "    'mm_grad' : 'av'\n",
    "}\n",
    "\n",
    "logits_results = {\n",
    "    video_id : {\n",
    "        modality_name_map[modality] : modality_predictions['logits'][i]\n",
    "        for modality, modality_predictions in modality_predictions.items()\n",
    "    }\n",
    "    for i, video_id in enumerate(video_ids)\n",
    "}\n",
    "\n",
    "pickle.dump(logits_results, open(\"logits.pkl\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avsiam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
